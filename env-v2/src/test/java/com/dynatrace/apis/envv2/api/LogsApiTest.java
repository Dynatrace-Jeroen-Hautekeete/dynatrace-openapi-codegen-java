/*
 * Dynatrace Environment API
 *  Documentation of the Dynatrace Environment API v2. Resources here generally supersede those in v1. Migration of resources from v1 is in progress.   If you miss a resource, consider using the Dynatrace Environment API v1. To read about use cases and examples, see [Dynatrace Documentation](https://dt-url.net/2u23k1k) .  Notes about compatibility: * Operations marked as early adopter or preview may be changed in non-compatible ways, although we try to avoid this. * We may add new enum constants without incrementing the API version; thus, clients need to handle unknown enum constants gracefully.
 *
 * The version of the OpenAPI document: 2.0.0
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package com.dynatrace.apis.envv2.api;

import com.dynatrace.apis.envv2.ApiException;
import com.dynatrace.apis.envv2.model.AggregatedLog;
import com.dynatrace.apis.envv2.model.ErrorEnvelope;
import com.dynatrace.apis.envv2.model.ExportedLogRecordList;
import com.dynatrace.apis.envv2.model.LogRecordsList;
import com.dynatrace.apis.envv2.model.SuccessEnvelope;
import org.junit.Test;
import org.junit.Ignore;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;


/**
 * API tests for LogsApi
 */
@Ignore
public class LogsApiTest {

    private final LogsApi api = new LogsApi();

    
    /**
     * Exports log records | maturity&#x3D;EARLY_ADOPTER
     *
     * Returns the first *X* records (specified in the **pageSize** query parameter).   Unlike the **search** API, this API does not split the result into slices and has no limit for the total number of records. Log records are sorted by the criteria specified in the **sort** query parameter.   In order to fetch large amount of records (exceeding the **pageSize** value), one should repeat the **export** call with **nextPageKey** param.  
     *
     * @throws ApiException
     *          if the Api call fails
     */
    @Test
    public void exportLogRecordsTest() throws ApiException {
        String from = null;
        String to = null;
        String nextPageKey = null;
        Long pageSize = null;
        String query = null;
        String sort = null;
        ExportedLogRecordList response = 
        api.exportLogRecords(from, to, nextPageKey, pageSize, query, sort);
        
        // TODO: test validations
    }
    
    /**
     * Gets aggregated log records | maturity&#x3D;EARLY_ADOPTER
     *
     * Returns the aggregated number of occurrences of log values divided into time slots.   It is possible that the timeframe covered by results exceeds the specified timeframe. In that case the request returns fewer time slots than specified in the **timeBuckets** query parameter.
     *
     * @throws ApiException
     *          if the Api call fails
     */
    @Test
    public void getLogHistogramDataTest() throws ApiException {
        String from = null;
        String to = null;
        String query = null;
        Integer timeBuckets = null;
        Integer maxGroupValues = null;
        List<String> groupBy = null;
        AggregatedLog response = 
        api.getLogHistogramData(from, to, query, timeBuckets, maxGroupValues, groupBy);
        
        // TODO: test validations
    }
    
    /**
     * Reads log records | maturity&#x3D;EARLY_ADOPTER
     *
     * Returns the first *X* records (specified in the **limit** query parameter). Log records are sorted by the criteria specified in the **sort** query parameter.   If the query is too large to be processed in a single request, it is divided into slices. In that case the first response contains the **nextSliceKey** cursor for the second slice. Use it in the **nextSliceKey** query parameter to obtain the second slice, which contains **nextSliceKey** cursor for the third slice, and so on.   Results can be distributed unevenly between slices and some slices might be empty.
     *
     * @throws ApiException
     *          if the Api call fails
     */
    @Test
    public void getLogRecordsTest() throws ApiException {
        String from = null;
        String to = null;
        Integer limit = null;
        String query = null;
        String sort = null;
        String nextSliceKey = null;
        LogRecordsList response = 
        api.getLogRecords(from, to, limit, query, sort, nextSliceKey);
        
        // TODO: test validations
    }
    
    /**
     * Pushes log records to Dynatrace | maturity&#x3D;EARLY_ADOPTER
     *
     * Ingested logs are stored in the indexed log storage.   This endpoint requires an ActiveGate with the **Log Analytics Collector** module enabled.   The maximum payload size of a single request is **5 MB**. Requests with a greater payload are rejected, and the API returns a 413 response code.  If the ingested payload is a JSON array, the maximum array size is **5000**. Requests with a greater payload are rejected, and the API returns a 413 response code.  &lt;br /&gt;**Log events per minute (SaaS)**:   Trial tenants: 10k, paid one: 100k per minute by default.   If your log data stream within your cluster exceeds the limit, all log events above the limit are ignored.   &lt;br /&gt;**Log events per minute (Managed)**:   10k/minute per cluster by default.   If your log data stream within your cluster exceeds the limit, all log events above the limit are ignored.   If you increase resources (RAM) in your nodes, you can increase the limit based on the cluster resources size using an API call or Cluster Management Console (CMC).   &lt;br /&gt;Refresh cluster limit using the API call   See [Update log events per cluster for Log Monitoring](https://dt-url.net/f123yeu).   &lt;br /&gt;Refresh cluster limit using Cluster Management Console (CMC)   1. In the CMC, select **Environments** and the environment for which you wish to update the total log events per cluster.   2. On the environment details page, in the **Cluster overload prevention settings** section, select the **Refresh cluster limit**.   **High-cardinality attributes:**   Unique log data attributes (high-cardinality attributes) such as &#x60;span_id&#x60; and &#x60;trace_id&#x60; generate unnecessarily excessive facet lists that may impact log viewer performance. Because of this, they aren&#39;t listed in log viewer facets. You can still use them in a log viewer advanced search query.   
     *
     * @throws ApiException
     *          if the Api call fails
     */
    @Test
    public void storeLogTest() throws ApiException {
        Object body = null;
        SuccessEnvelope response = 
        api.storeLog(body);
        
        // TODO: test validations
    }
    
}
